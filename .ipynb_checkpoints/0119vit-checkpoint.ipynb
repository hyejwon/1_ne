{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2e576f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f83870246de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model parameter 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'name:{name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'param.shape:{param.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#model parameter 확인\n",
    "for name, param in model.named_parameters(): \n",
    "    print(f'name:{name}') \n",
    "    print(type(param)) \n",
    "    print(f'param.shape:{param.shape}') \n",
    "    print(f'param.requries_grad:{param.requires_grad}') \n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51394771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458e1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.patch_embed.proj=nn.Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cddae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db6042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "#sys.path.append(\"..\")\n",
    "from tools import utils\n",
    "#from model.video_kwsnet import KWS_Net\n",
    "from reader.data_reader_video import myDataLoader, myDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfb7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f295bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset ...\n",
      "- done.\n",
      "- 51269 training samples, 2705 dev samples\n",
      "- 12818 training batch, 677 dev batch\n"
     ]
    }
   ],
   "source": [
    "model_path = 'video_model'\n",
    "log_dir = './log/'\n",
    "logger = utils.get_logger(log_dir + '/' + 'video_model')\n",
    "seed_torch(617)\n",
    "\n",
    "# load mean and var\n",
    "lip_train_mean_var = np.load('scp_dir/train_mean_var_lip.npz')\n",
    "lip_train_mean = lip_train_mean_var['_mean']\n",
    "lip_train_var = lip_train_mean_var['_var']\n",
    "\n",
    "# file path\n",
    "file_train_positive_path  = '/home/hj20/misp2021_baseline/task1_wws/kws_net_only_video/scp_dir/positive_train.scp'\n",
    "file_train_negative_path  = '/home/hj20/misp2021_baseline/task1_wws/kws_net_only_video/scp_dir/negative_train.scp'\n",
    "file_dev_positive_path  = '/home/hj20/misp2021_baseline/task1_wws/kws_net_only_video/scp_dir/positive_dev.scp'\n",
    "file_dev_negative_path = '/home/hj20/misp2021_baseline/task1_wws/kws_net_only_video/scp_dir/negative_dev.scp'\n",
    "    \n",
    "# define the dataloader\n",
    "print(\"loading the dataset ...\")\n",
    "dataset_train = myDataset(file_train_positive_path, file_train_negative_path, lip_train_mean, lip_train_var)\n",
    "dataset_dev = myDataset(file_dev_positive_path, file_dev_negative_path, lip_train_mean, lip_train_var)\n",
    "dataloader_train = myDataLoader(dataset=dataset_train,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            num_workers=6)\n",
    "dataloader_dev = myDataLoader(dataset=dataset_dev,\n",
    "                            batch_size=4,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1)\n",
    "print(\"- done.\")\n",
    "all_file = len(dataloader_train)\n",
    "all_file_dev = len(dataloader_dev)\n",
    "print(\"- {} training samples, {} dev samples\".format(len(dataset_train), len(dataset_dev)))\n",
    "print(\"- {} training batch, {} dev batch\".format(len(dataloader_train), len(dataloader_dev)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb070789",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-226266991b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'torch'"
     ]
    }
   ],
   "source": [
    "x= x.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb44e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset_train[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06b3afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd881bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c948d32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33, 96, 96])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e97bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frontend = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False),\n",
    "            nn.BatchNorm3d(16), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c90eb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, _, _ = x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "307d3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31e9e6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 33, 96, 96])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0e9da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frontend=video_frontend.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8608d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = video_frontend(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51561b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 33, 24, 24])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "275f1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c078db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 34, 64, 24, 24])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a980dc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[6.7833e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 6.1398e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 6.1398e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 5.6885e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 1.2454e-01, 1.0277e-01,  ..., 2.0375e+00,\n",
       "            2.3183e+00, 2.4504e+00],\n",
       "           [3.0174e-01, 6.9994e-01, 7.7454e-01,  ..., 2.1981e+00,\n",
       "            2.3183e+00, 2.4504e+00],\n",
       "           [5.4947e-01, 5.9432e-01, 5.9432e-01,  ..., 2.5050e+00,\n",
       "            2.5098e+00, 2.5456e+00]],\n",
       "\n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.6002e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4307e-01, 3.8793e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.4681e+00, 1.4681e+00, 1.4872e+00,  ..., 3.0357e+00,\n",
       "            2.8613e+00, 2.8686e+00],\n",
       "           [1.4681e+00, 1.4681e+00, 1.4872e+00,  ..., 3.0357e+00,\n",
       "            2.8848e+00, 2.9351e+00],\n",
       "           [9.0045e-01, 9.6814e-01, 9.6814e-01,  ..., 2.6558e+00,\n",
       "            2.5939e+00, 2.5789e+00]],\n",
       "\n",
       "          [[1.4447e+00, 6.9037e-01, 6.0623e-02,  ..., 2.2986e-01,\n",
       "            4.2839e-02, 7.6658e-02],\n",
       "           [1.4447e+00, 6.9037e-01, 6.0623e-02,  ..., 2.2986e-01,\n",
       "            4.2839e-02, 7.6658e-02],\n",
       "           [6.2076e-01, 1.9808e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.3504e-01, 0.0000e+00, 8.9193e-01,  ..., 1.3662e+00,\n",
       "            1.7139e+00, 2.2669e+00],\n",
       "           [2.6862e-01, 0.0000e+00, 5.6778e-01,  ..., 1.1065e+00,\n",
       "            1.1065e+00, 3.5081e+00],\n",
       "           [8.0877e-02, 2.5345e-01, 2.8042e-01,  ..., 1.3583e+00,\n",
       "            1.3778e+00, 3.5081e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[1.2673e+00, 1.1014e+00, 1.0470e+00,  ..., 1.1666e+00,\n",
       "            1.0471e+00, 1.1019e+00],\n",
       "           [1.3966e+00, 5.8043e-01, 5.7239e-01,  ..., 6.9780e-01,\n",
       "            6.9780e-01, 5.9956e-01],\n",
       "           [1.4182e+00, 5.5533e-01, 5.7760e-01,  ..., 6.9780e-01,\n",
       "            6.9780e-01, 5.9956e-01],\n",
       "           ...,\n",
       "           [1.4787e+00, 1.5356e+00, 1.5356e+00,  ..., 7.1607e-01,\n",
       "            6.5856e-01, 2.2885e+00],\n",
       "           [1.2211e+00, 9.3258e-01, 1.1052e+00,  ..., 4.8708e-01,\n",
       "            5.4208e-01, 6.9543e-01],\n",
       "           [6.6982e-01, 7.5236e-01, 6.1347e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 2.0498e-01]],\n",
       "\n",
       "          [[0.0000e+00, 2.6848e-01, 2.5924e-01,  ..., 3.3877e-01,\n",
       "            4.5328e-01, 4.5328e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 4.8191e-02,  ..., 3.8055e+00,\n",
       "            3.6248e+00, 4.8738e+00],\n",
       "           [5.7644e-01, 1.0532e+00, 1.2359e+00,  ..., 3.8055e+00,\n",
       "            3.6248e+00, 4.8738e+00],\n",
       "           [5.7644e-01, 1.0532e+00, 1.2359e+00,  ..., 2.8021e+00,\n",
       "            2.8021e+00, 4.6423e+00]]],\n",
       "\n",
       "\n",
       "         [[[1.8169e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 4.3041e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 6.1188e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 5.5971e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0022e+00,\n",
       "            1.0022e+00, 8.5050e-01],\n",
       "           [8.8186e-01, 9.9997e-01, 9.2197e-01,  ..., 1.1021e+00,\n",
       "            1.1021e+00, 1.0934e+00],\n",
       "           [8.8186e-01, 9.9997e-01, 9.2197e-01,  ..., 1.0627e+00,\n",
       "            1.0210e+00, 1.0934e+00]],\n",
       "\n",
       "          [[4.3333e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.0574e-01, 6.6071e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.9451e-01, 6.0950e-01, 0.0000e+00,  ..., 2.8854e-01,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0744e+00, 8.4518e-01, 3.3862e-01,  ..., 9.2930e-01,\n",
       "            8.6441e-01, 1.1027e+00],\n",
       "           [7.2786e-01, 6.8281e-01, 2.8368e-01,  ..., 1.0089e+00,\n",
       "            1.0188e+00, 1.7441e+00],\n",
       "           [7.2786e-01, 6.8281e-01, 6.6620e-01,  ..., 8.6905e-01,\n",
       "            8.8402e-01, 1.7441e+00]],\n",
       "\n",
       "          [[1.9831e+00, 1.6153e+00, 1.5260e+00,  ..., 1.7824e+00,\n",
       "            1.7824e+00, 1.7668e+00],\n",
       "           [1.9831e+00, 1.6153e+00, 1.5260e+00,  ..., 1.7824e+00,\n",
       "            1.7824e+00, 1.7668e+00],\n",
       "           [1.3353e+00, 8.1886e-01, 3.1386e-01,  ..., 4.9206e-01,\n",
       "            5.9066e-01, 5.9066e-01],\n",
       "           ...,\n",
       "           [1.4217e+00, 9.2753e-01, 4.0096e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.7459e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.5101e+00],\n",
       "           [0.0000e+00, 1.6852e-02, 6.8674e-02,  ..., 1.1901e+00,\n",
       "            1.1960e+00, 2.8617e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[1.2184e+00, 1.3439e+00, 1.2910e+00,  ..., 1.2951e+00,\n",
       "            1.2298e+00, 1.3374e+00],\n",
       "           [7.7990e-01, 7.7990e-01, 4.3327e-01,  ..., 2.5384e-01,\n",
       "            3.7745e-01, 5.4649e-01],\n",
       "           [6.8155e-01, 6.8155e-01, 3.0661e-01,  ..., 3.0617e-01,\n",
       "            3.7745e-01, 3.8799e-01],\n",
       "           ...,\n",
       "           [1.3046e+00, 1.1397e+00, 3.2692e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.1415e-01],\n",
       "           [7.6223e-01, 2.6854e-01, 6.4512e-01,  ..., 0.0000e+00,\n",
       "            3.6540e-02, 1.4781e+00],\n",
       "           [8.9656e-02, 6.0057e-01, 6.0057e-01,  ..., 1.7464e-01,\n",
       "            1.8388e-01, 1.4333e+00]],\n",
       "\n",
       "          [[0.0000e+00, 4.0245e-01, 3.8126e-01,  ..., 4.5721e-01,\n",
       "            4.2145e-01, 3.6800e-01],\n",
       "           [0.0000e+00, 3.4792e-02, 7.8237e-03,  ..., 1.2993e-01,\n",
       "            4.6944e-02, 3.3852e-02],\n",
       "           [0.0000e+00, 4.5300e-02, 1.9660e-02,  ..., 1.2993e-01,\n",
       "            5.8820e-02, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 1.2257e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.1332e-01],\n",
       "           [0.0000e+00, 8.6020e-02, 2.9402e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.4341e-01]],\n",
       "\n",
       "          [[3.6783e-01, 4.5045e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6783e-01, 2.7032e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8217e-01, 2.8217e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.5770e-01, 2.5770e-01, 1.1082e-02,  ..., 1.2099e+00,\n",
       "            1.1184e+00, 3.0210e+00],\n",
       "           [8.7375e-01, 8.7375e-01, 4.8696e-01,  ..., 1.1278e+00,\n",
       "            1.2092e+00, 4.1631e+00],\n",
       "           [2.9018e-01, 2.9018e-01, 0.0000e+00,  ..., 1.1278e+00,\n",
       "            1.2092e+00, 4.2023e+00]]],\n",
       "\n",
       "\n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 7.6154e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.6576e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7162e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 1.3280e+00, 1.5814e+00,  ..., 3.3186e+00,\n",
       "            3.0648e+00, 3.3851e+00],\n",
       "           [9.0324e-01, 1.5813e+00, 1.5814e+00,  ..., 1.8452e+00,\n",
       "            1.9005e+00, 2.0292e+00],\n",
       "           [6.1957e-01, 7.6409e-01, 6.5887e-01,  ..., 1.8452e+00,\n",
       "            1.9005e+00, 2.0292e+00]],\n",
       "\n",
       "          [[1.1770e+00, 4.7516e-01, 1.6909e-01,  ..., 3.0714e-01,\n",
       "            3.3298e-01, 3.3298e-01],\n",
       "           [1.1770e+00, 4.7516e-01, 1.6909e-01,  ..., 3.0714e-01,\n",
       "            3.3298e-01, 3.3298e-01],\n",
       "           [8.4112e-01, 1.0746e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.6706e-01, 0.0000e+00, 0.0000e+00,  ..., 3.1068e+00,\n",
       "            3.3886e+00, 4.6058e+00],\n",
       "           [7.2333e-01, 1.2566e+00, 2.1347e+00,  ..., 3.1068e+00,\n",
       "            3.3886e+00, 4.6058e+00],\n",
       "           [7.2333e-01, 1.2566e+00, 2.1347e+00,  ..., 1.5010e+00,\n",
       "            1.5010e+00, 2.4664e+00]],\n",
       "\n",
       "          [[1.4768e+00, 1.1071e+00, 1.1071e+00,  ..., 1.1253e+00,\n",
       "            1.0691e+00, 1.3283e+00],\n",
       "           [1.4768e+00, 1.1071e+00, 1.1071e+00,  ..., 1.1253e+00,\n",
       "            1.0691e+00, 1.3283e+00],\n",
       "           [7.9341e-01, 3.9627e-01, 2.0216e-01,  ..., 3.6895e-01,\n",
       "            2.9757e-01, 3.1481e-01],\n",
       "           ...,\n",
       "           [7.9833e-01, 4.1742e-01, 1.9936e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.1162e+00],\n",
       "           [4.0235e-01, 8.6051e-02, 4.0510e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7764e+00],\n",
       "           [0.0000e+00, 1.7445e-02, 4.0510e-01,  ..., 8.8105e-01,\n",
       "            7.0843e-01, 1.5494e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[2.9293e+00, 2.9293e+00, 2.2957e+00,  ..., 2.7572e+00,\n",
       "            2.6586e+00, 2.3265e+00],\n",
       "           [1.6007e+00, 1.6007e+00, 1.9463e-01,  ..., 2.5812e-01,\n",
       "            7.6202e-02, 0.0000e+00],\n",
       "           [1.5915e+00, 1.5915e+00, 2.9336e-01,  ..., 2.5812e-01,\n",
       "            7.6202e-02, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.3683e+00, 1.3683e+00, 3.1119e-01,  ..., 1.8402e+00,\n",
       "            2.2276e+00, 2.9166e+00],\n",
       "           [6.9857e-01, 1.0283e+00, 1.7671e+00,  ..., 1.2910e+00,\n",
       "            1.6509e+00, 1.7798e+00],\n",
       "           [1.1551e+00, 1.1551e+00, 6.5992e-01,  ..., 1.5563e+00,\n",
       "            1.6956e+00, 1.6956e+00]],\n",
       "\n",
       "          [[3.3744e-01, 8.9403e-01, 8.8453e-01,  ..., 1.1503e+00,\n",
       "            1.1503e+00, 1.0636e+00],\n",
       "           [0.0000e+00, 9.0484e-01, 9.0797e-01,  ..., 8.2266e-01,\n",
       "            9.2709e-01, 9.7072e-01],\n",
       "           [0.0000e+00, 9.0484e-01, 9.0797e-01,  ..., 9.1773e-01,\n",
       "            9.2709e-01, 9.3611e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 8.3045e-01, 1.1160e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 2.8019e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          [[2.9974e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.7610e-01, 7.7610e-01, 1.2506e-01,  ..., 4.5093e-01,\n",
       "            4.5093e-01, 4.3065e-01],\n",
       "           [8.7972e-01, 8.7972e-01, 3.0576e-01,  ..., 3.6517e-01,\n",
       "            4.2705e-01, 4.2705e-01],\n",
       "           ...,\n",
       "           [1.2965e+00, 1.2965e+00, 1.4297e+00,  ..., 4.1132e-01,\n",
       "            1.8782e-01, 3.6539e+00],\n",
       "           [1.1993e+00, 1.1993e+00, 5.5881e-01,  ..., 4.1132e-01,\n",
       "            1.8782e-01, 3.7132e+00],\n",
       "           [1.1993e+00, 1.1993e+00, 5.5881e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.2934e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 8.5582e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.6882e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7353e+00],\n",
       "           ...,\n",
       "           [1.6895e-01, 1.6895e-01, 1.9444e-01,  ..., 1.5300e+00,\n",
       "            1.6469e+00, 1.8343e+00],\n",
       "           [1.6895e-01, 7.2450e-01, 7.2450e-01,  ..., 1.5300e+00,\n",
       "            1.6469e+00, 1.8343e+00],\n",
       "           [1.7943e-01, 7.2450e-01, 7.2450e-01,  ..., 1.5892e+00,\n",
       "            1.5353e+00, 1.4947e+00]],\n",
       "\n",
       "          [[1.1470e+00, 4.5253e-01, 1.9099e-01,  ..., 1.9992e-01,\n",
       "            2.0061e-01, 2.1594e-01],\n",
       "           [1.1470e+00, 4.5253e-01, 1.9099e-01,  ..., 1.9992e-01,\n",
       "            2.0061e-01, 2.1594e-01],\n",
       "           [7.0621e-01, 5.4768e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.9398e-01, 0.0000e+00, 0.0000e+00,  ..., 2.4390e+00,\n",
       "            2.4390e+00, 2.1264e+00],\n",
       "           [1.4432e+00, 1.7658e+00, 1.8791e+00,  ..., 2.5184e+00,\n",
       "            2.9651e+00, 4.1278e+00],\n",
       "           [1.5772e+00, 1.8585e+00, 2.0722e+00,  ..., 1.4706e+00,\n",
       "            1.3792e+00, 2.2159e+00]],\n",
       "\n",
       "          [[1.5053e+00, 1.0969e+00, 1.0277e+00,  ..., 1.0871e+00,\n",
       "            1.1588e+00, 1.2141e+00],\n",
       "           [1.5053e+00, 1.0969e+00, 1.0277e+00,  ..., 1.0871e+00,\n",
       "            1.1588e+00, 1.2141e+00],\n",
       "           [8.1339e-01, 4.2512e-01, 1.9172e-01,  ..., 0.0000e+00,\n",
       "            1.5747e-01, 1.5747e-01],\n",
       "           ...,\n",
       "           [5.7089e-01, 7.5821e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.7818e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            4.4954e-01, 2.1089e+00],\n",
       "           [3.2330e-01, 9.0119e-01, 9.9919e-01,  ..., 4.0192e-01,\n",
       "            4.4954e-01, 2.1089e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[3.0439e+00, 3.0439e+00, 2.3293e+00,  ..., 2.6015e+00,\n",
       "            2.5447e+00, 2.2664e+00],\n",
       "           [1.7610e+00, 1.7610e+00, 9.1892e-02,  ..., 1.4629e-01,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6040e+00, 1.6040e+00, 1.0547e-01,  ..., 3.7615e-02,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.8785e-01, 6.8785e-01, 0.0000e+00,  ..., 1.7406e+00,\n",
       "            1.8737e+00, 2.7849e+00],\n",
       "           [1.0213e+00, 1.5645e+00, 1.9189e+00,  ..., 1.7406e+00,\n",
       "            1.8737e+00, 2.7849e+00],\n",
       "           [1.0213e+00, 1.8277e+00, 1.6999e+00,  ..., 1.5267e+00,\n",
       "            1.4834e+00, 1.4029e+00]],\n",
       "\n",
       "          [[4.4044e-01, 9.7048e-01, 9.5471e-01,  ..., 1.0734e+00,\n",
       "            1.0074e+00, 9.4019e-01],\n",
       "           [0.0000e+00, 8.4832e-01, 8.1303e-01,  ..., 9.4772e-01,\n",
       "            9.9092e-01, 1.0124e+00],\n",
       "           [0.0000e+00, 7.9837e-01, 7.9572e-01,  ..., 9.4772e-01,\n",
       "            9.8613e-01, 1.0124e+00],\n",
       "           ...,\n",
       "           [7.7898e-02, 9.3183e-01, 9.0170e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 3.5538e-01, 7.7788e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          [[3.0700e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.6863e-01, 7.6863e-01, 1.4838e-01,  ..., 3.1545e-01,\n",
       "            3.1437e-01, 3.1437e-01],\n",
       "           [7.6863e-01, 7.6863e-01, 1.9126e-01,  ..., 3.1545e-01,\n",
       "            3.0770e-01, 2.2006e-01],\n",
       "           ...,\n",
       "           [1.1384e+00, 1.1384e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 8.6499e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.0603e+00],\n",
       "           [5.4521e-01, 5.4521e-01, 3.1733e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.0603e+00]]],\n",
       "\n",
       "\n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.0227e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7338e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7338e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 5.8720e-01, 5.8720e-01,  ..., 1.8338e+00,\n",
       "            1.8515e+00, 1.6635e+00],\n",
       "           [4.2098e-01, 1.2529e+00, 1.0801e+00,  ..., 3.0825e+00,\n",
       "            3.0530e+00, 2.9367e+00],\n",
       "           [1.1717e+00, 1.8859e+00, 1.8859e+00,  ..., 2.2649e+00,\n",
       "            2.5923e+00, 2.9367e+00]],\n",
       "\n",
       "          [[1.4006e+00, 7.1636e-01, 6.8038e-01,  ..., 6.9681e-01,\n",
       "            6.7798e-01, 6.6592e-01],\n",
       "           [1.4006e+00, 7.1636e-01, 6.8038e-01,  ..., 6.9681e-01,\n",
       "            6.7798e-01, 6.6592e-01],\n",
       "           [6.4408e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.5956e-01, 3.2239e-02, 1.4292e-01,  ..., 3.6982e+00,\n",
       "            4.1383e+00, 4.1383e+00],\n",
       "           [9.0354e-01, 2.3446e+00, 2.6494e+00,  ..., 9.0486e-01,\n",
       "            1.0569e+00, 2.0468e+00],\n",
       "           [6.8386e-01, 3.3422e-01, 4.9551e-01,  ..., 1.6107e+00,\n",
       "            1.6133e+00, 2.3252e+00]],\n",
       "\n",
       "          [[4.1588e-01, 9.1212e-02, 9.1212e-02,  ..., 2.6972e-01,\n",
       "            6.2868e-02, 0.0000e+00],\n",
       "           [5.2947e-01, 3.2050e-01, 2.1536e-01,  ..., 5.9262e-01,\n",
       "            4.6339e-01, 3.7580e-01],\n",
       "           [5.3501e-01, 3.6533e-01, 2.8846e-01,  ..., 9.3695e-01,\n",
       "            7.6873e-01, 5.0051e-01],\n",
       "           ...,\n",
       "           [9.4125e-01, 1.2533e+00, 1.4406e+00,  ..., 2.0519e+00,\n",
       "            2.0614e+00, 2.3335e+00],\n",
       "           [1.0683e+00, 1.4427e+00, 1.9412e+00,  ..., 8.2440e-01,\n",
       "            9.3726e-01, 1.0609e+00],\n",
       "           [6.7814e-01, 6.7814e-01, 6.8709e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.0411e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[2.8206e+00, 2.8206e+00, 1.9682e+00,  ..., 2.1138e+00,\n",
       "            2.2034e+00, 2.0692e+00],\n",
       "           [2.1604e+00, 2.1604e+00, 1.0311e-01,  ..., 1.3610e-01,\n",
       "            2.6375e-01, 2.6375e-01],\n",
       "           [2.0834e+00, 2.0834e+00, 1.2668e-01,  ..., 1.3610e-01,\n",
       "            1.4666e-01, 1.4666e-01],\n",
       "           ...,\n",
       "           [2.0691e+00, 2.0691e+00, 7.7053e-01,  ..., 8.6080e-01,\n",
       "            9.8191e-01, 7.8436e-01],\n",
       "           [1.8818e+00, 1.8818e+00, 7.7053e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1616e-01, 2.1616e-01, 2.3726e-01,  ..., 1.0632e+00,\n",
       "            1.2062e+00, 1.8246e+00]],\n",
       "\n",
       "          [[1.1997e+00, 1.7377e+00, 1.7050e+00,  ..., 1.9125e+00,\n",
       "            1.9236e+00, 1.9072e+00],\n",
       "           [1.3278e+00, 1.8979e+00, 1.8174e+00,  ..., 1.9582e+00,\n",
       "            2.0324e+00, 2.0258e+00],\n",
       "           [1.3001e+00, 1.8574e+00, 1.8012e+00,  ..., 1.8718e+00,\n",
       "            1.9998e+00, 1.9998e+00],\n",
       "           ...,\n",
       "           [1.3899e+00, 1.8887e+00, 1.7624e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.5066e-01, 6.8644e-01, 5.8845e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 5.1919e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          [[2.5721e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6119e-01, 3.6119e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.4682e-01, 4.4682e-01, 0.0000e+00,  ..., 6.8709e-02,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.6332e-01, 5.6332e-01, 0.0000e+00,  ..., 1.0351e+00,\n",
       "            1.2418e+00, 2.6639e+00],\n",
       "           [1.0416e+00, 1.0416e+00, 7.5440e-01,  ..., 1.0351e+00,\n",
       "            1.2418e+00, 2.6639e+00],\n",
       "           [1.0416e+00, 1.0416e+00, 7.5440e-01,  ..., 6.2218e-01,\n",
       "            7.3071e-01, 2.4441e+00]]],\n",
       "\n",
       "\n",
       "         [[[0.0000e+00, 5.4712e-01, 5.5828e-01,  ..., 6.3859e-01,\n",
       "            6.3859e-01, 1.4864e+00],\n",
       "           [0.0000e+00, 9.1226e-01, 9.0097e-01,  ..., 9.2500e-01,\n",
       "            9.2500e-01, 2.1880e+00],\n",
       "           [0.0000e+00, 9.4759e-01, 9.1395e-01,  ..., 8.8251e-01,\n",
       "            9.1795e-01, 2.2362e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 1.1936e+00, 1.2237e+00,  ..., 1.3937e+00,\n",
       "            1.2351e+00, 1.2947e+00],\n",
       "           [8.9626e-01, 9.5871e-01, 1.2237e+00,  ..., 9.6631e-01,\n",
       "            1.0290e+00, 1.0290e+00],\n",
       "           [1.2431e+00, 1.3289e+00, 1.4192e+00,  ..., 9.6631e-01,\n",
       "            1.0290e+00, 1.0290e+00]],\n",
       "\n",
       "          [[1.9868e+00, 1.9088e+00, 1.8242e+00,  ..., 2.0763e+00,\n",
       "            1.9658e+00, 1.9473e+00],\n",
       "           [1.9868e+00, 1.9088e+00, 1.8242e+00,  ..., 2.0763e+00,\n",
       "            1.9658e+00, 1.9473e+00],\n",
       "           [8.0624e-01, 5.1394e-01, 5.4680e-01,  ..., 6.9132e-01,\n",
       "            7.7727e-01, 7.7727e-01],\n",
       "           ...,\n",
       "           [9.0236e-01, 6.9261e-01, 8.3424e-01,  ..., 5.6982e-01,\n",
       "            5.9632e-01, 1.1176e+00],\n",
       "           [5.1399e-01, 5.2128e-01, 8.7221e-01,  ..., 3.9532e-02,\n",
       "            1.1225e-01, 1.1176e+00],\n",
       "           [3.0893e-01, 8.4878e-02, 1.1108e-01,  ..., 7.1012e-01,\n",
       "            7.9427e-01, 1.6432e+00]],\n",
       "\n",
       "          [[6.0084e-01, 1.3386e-01, 1.5914e-01,  ..., 3.0824e-01,\n",
       "            3.0824e-01, 1.8576e-01],\n",
       "           [9.4150e-01, 8.3637e-01, 8.1377e-01,  ..., 9.9188e-01,\n",
       "            9.9188e-01, 9.3588e-01],\n",
       "           [9.3351e-01, 8.0244e-01, 7.6144e-01,  ..., 9.0028e-01,\n",
       "            9.3588e-01, 9.3588e-01],\n",
       "           ...,\n",
       "           [1.1217e+00, 8.8738e-01, 7.6652e-01,  ..., 9.7589e-01,\n",
       "            8.1355e-01, 9.3585e-01],\n",
       "           [7.2244e-01, 1.1105e+00, 1.6132e+00,  ..., 9.7589e-01,\n",
       "            8.1355e-01, 1.3573e+00],\n",
       "           [7.0075e-01, 1.1105e+00, 1.6132e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[3.2593e+00, 3.2593e+00, 2.3683e+00,  ..., 2.5320e+00,\n",
       "            2.4836e+00, 2.3568e+00],\n",
       "           [3.4342e+00, 3.4342e+00, 1.0604e+00,  ..., 1.1894e+00,\n",
       "            1.1894e+00, 1.1830e+00],\n",
       "           [3.4125e+00, 3.4125e+00, 9.5367e-01,  ..., 1.0647e+00,\n",
       "            1.1656e+00, 1.1656e+00],\n",
       "           ...,\n",
       "           [3.6894e+00, 3.6894e+00, 1.1411e+00,  ..., 4.9835e-01,\n",
       "            1.5827e-01, 0.0000e+00],\n",
       "           [3.3539e+00, 3.3539e+00, 8.4036e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2693e+00, 1.2693e+00, 0.0000e+00,  ..., 4.6375e-01,\n",
       "            5.0109e-01, 9.4579e-01]],\n",
       "\n",
       "          [[5.2185e-01, 1.1708e+00, 1.1447e+00,  ..., 1.3359e+00,\n",
       "            1.2721e+00, 1.1815e+00],\n",
       "           [7.0725e-01, 1.3400e+00, 1.2999e+00,  ..., 1.4387e+00,\n",
       "            1.4387e+00, 1.3829e+00],\n",
       "           [6.8764e-01, 1.3400e+00, 1.2822e+00,  ..., 1.4039e+00,\n",
       "            1.4137e+00, 1.4015e+00],\n",
       "           ...,\n",
       "           [6.6971e-01, 1.2744e+00, 1.2006e+00,  ..., 1.8156e-01,\n",
       "            9.1815e-02, 1.0312e-01],\n",
       "           [4.9622e-01, 1.0960e+00, 9.9594e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.2756e-02, 2.3095e-01, 2.2745e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "          [[5.2290e-01, 6.7957e-03, 3.3147e-02,  ..., 2.7841e-02,\n",
       "            2.7841e-02, 0.0000e+00],\n",
       "           [1.3617e+00, 1.3617e+00, 9.5966e-01,  ..., 1.1988e+00,\n",
       "            1.0586e+00, 1.0414e+00],\n",
       "           [1.3766e+00, 1.3766e+00, 9.8856e-01,  ..., 1.2641e+00,\n",
       "            1.1745e+00, 1.1572e+00],\n",
       "           ...,\n",
       "           [1.4120e+00, 1.4120e+00, 1.2432e+00,  ..., 1.1852e+00,\n",
       "            1.0073e+00, 9.3725e-01],\n",
       "           [1.4120e+00, 1.4120e+00, 1.3552e+00,  ..., 5.0911e-01,\n",
       "            5.0661e-01, 1.7288e+00],\n",
       "           [1.3047e+00, 1.3345e+00, 1.3552e+00,  ..., 2.1884e-01,\n",
       "            5.0661e-01, 1.7288e+00]]]]], device='cuda:0',\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "257cd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PaSST().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbf9117a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 64, 24, 24])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4aed9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d99b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(-1, 64, x.size(3), x.size(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fff8cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 64, 24, 24])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef7b5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.cuda of ResNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bnfc): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5796e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2961211d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cuda(): argument 'device' (position 1) must be torch.device, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4c8d3059cdcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cuda(): argument 'device' (position 1) must be torch.device, not Tensor"
     ]
    }
   ],
   "source": [
    "resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e3b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390521ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c65657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686a25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers.helpers import to_2tuple\n",
    "\n",
    "#from .helpers.vit_helpers import update_default_cfg_and_kwargs, DropPath, trunc_normal_, build_model_with_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4e9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_input_conv(in_chans, conv_weight):\n",
    "    conv_type = conv_weight.dtype\n",
    "    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n",
    "    O, I, J, K = conv_weight.shape\n",
    "    if in_chans == 1:\n",
    "        if I > 3:\n",
    "            assert conv_weight.shape[1] % 3 == 0\n",
    "            # For models with space2depth stems\n",
    "            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)\n",
    "            conv_weight = conv_weight.sum(dim=2, keepdim=False)\n",
    "        else:\n",
    "            conv_weight = conv_weight.sum(dim=1, keepdim=True)\n",
    "    elif in_chans != 3:\n",
    "        if I != 3:\n",
    "            raise NotImplementedError('Weight format not supported by conversion.')\n",
    "        else:\n",
    "            # NOTE this strategy should be better than random init, but there could be other combinations of\n",
    "            # the original RGB input layer weights that'd work better for specific cases.\n",
    "            repeat = int(math.ceil(in_chans / 3))\n",
    "            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n",
    "            conv_weight *= (3 / float(in_chans))\n",
    "    conv_weight = conv_weight.to(conv_type)\n",
    "    return conv_weight\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "first_RUN = True\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, norm_layer=None,\n",
    "                 flatten=True):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        stride = to_2tuple(stride)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.grid_size = (img_size[0] // stride[0], img_size[1] // stride[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if not (H == self.img_size[0] and W == self.img_size[1]):\n",
    "            warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
    "        # to do maybe replace weights\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        if first_RUN: print(\"self.norm(x)\", x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PaSST(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the implementation of Vision Transformer in timm library.\n",
    "     Take a look at the get_model function, adapting the weights of pretrained imagenet models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, u_patchout=0, s_patchout_t=0, s_patchout_f=0, img_size=(24, 24), patch_size=4, stride=4,\n",
    "                 in_chans=3, num_classes=256, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u_patchout: Unstructured Patchout integer, number of items to be removed from the final sequence\n",
    "            s_patchout_t: structured Patchout time integer, number of columns to be removed from the patches grid\n",
    "            s_patchout_f: structured Patchout Frequency integer, number of rows to be removed from the patches grid\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.u_patchout = u_patchout\n",
    "        self.s_patchout_t = s_patchout_t\n",
    "        self.s_patchout_f = s_patchout_f\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, stride=stride, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            flatten=False)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        # PaSST\n",
    "        # refer to https://arxiv.org/abs/2110.05069 Section 2\n",
    "        self.new_pos_embed = nn.Parameter(torch.zeros(1, self.num_tokens, embed_dim))  # for C and D tokens\n",
    "        self.freq_new_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.patch_embed.grid_size[0], 1))  # | f\n",
    "        self.time_new_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, 1, self.patch_embed.grid_size[1]))  # __ t\n",
    "        ####\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(self.num_features),\n",
    "                                  nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.new_pos_embed, std=.02)\n",
    "        trunc_normal_(self.freq_new_pos_embed, std=.02)\n",
    "        trunc_normal_(self.time_new_pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            raise RuntimeError(\"Not supported yet\")\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'new_pos_embed', 'freq_new_pos_embed', 'time_new_pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        global first_RUN  # not jit friendly? use trace instead\n",
    "        x = self.patch_embed(x)  # [b, e, f, t]\n",
    "        B_dim, E_dim, F_dim, T_dim = x.shape  # slow\n",
    "        if first_RUN: print(\" patch_embed : \", x.shape)\n",
    "        # Adding Time/Freq information\n",
    "        if first_RUN: print(\" self.time_new_pos_embed.shape\", self.time_new_pos_embed.shape)\n",
    "        time_new_pos_embed = self.time_new_pos_embed\n",
    "        if x.shape[-1] != time_new_pos_embed.shape[-1]:\n",
    "            time_new_pos_embed = time_new_pos_embed[:, :, :, :x.shape[-1]]\n",
    "            if first_RUN: print(\" CUT time_new_pos_embed.shape\", time_new_pos_embed.shape)\n",
    "        x = x + time_new_pos_embed\n",
    "        if first_RUN: print(\" self.freq_new_pos_embed.shape\", self.freq_new_pos_embed.shape)\n",
    "        x = x + self.freq_new_pos_embed\n",
    "\n",
    "        # Structured Patchout https://arxiv.org/abs/2110.05069 Section 2.2\n",
    "        if self.training and self.s_patchout_t:\n",
    "            if first_RUN: print(f\"X Before time Patchout of {self.s_patchout_t} \", x.size())\n",
    "            # ([1, 768, 1, 82])\n",
    "            random_indices = torch.randperm(T_dim)[:T_dim - self.s_patchout_t].sort().values\n",
    "            x = x[:, :, :, random_indices]\n",
    "            if first_RUN: print(\"X after time Patchout\", x.size())\n",
    "        if self.training and self.s_patchout_f:\n",
    "            if first_RUN: print(f\"X Before Freq Patchout of {self.s_patchout_f} \", x.size())\n",
    "            # [1, 768, 12, 1]\n",
    "            random_indices = torch.randperm(F_dim)[:F_dim - self.s_patchout_f].sort().values\n",
    "            x = x[:, :, random_indices, :]\n",
    "            if first_RUN: print(\" \\n X after freq Patchout: \", x.size())\n",
    "        ###\n",
    "        # Flatten the sequence\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # Unstructured Patchout\n",
    "        if first_RUN: print(\"X flattened\", x.size())\n",
    "        if self.training and self.u_patchout:\n",
    "            seq_len = x.shape[1]\n",
    "            random_indices = torch.randperm(seq_len)[:seq_len - self.u_patchout].sort().values\n",
    "            x = x[:, random_indices, :]\n",
    "            if first_RUN: print(\"X After Unstructured Patchout\", x.size())\n",
    "        ####\n",
    "        # Add the C/D tokens\n",
    "        if first_RUN: print(\" self.new_pos_embed.shape\", self.new_pos_embed.shape)\n",
    "        cls_tokens = self.cls_token.expand(B_dim, -1, -1) + self.new_pos_embed[:, :1, :]\n",
    "        if first_RUN: print(\" self.cls_tokens.shape\", cls_tokens.shape)\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "        else:\n",
    "            dist_token = self.dist_token.expand(B_dim, -1, -1) + self.new_pos_embed[:, 1:, :]\n",
    "            if first_RUN: print(\" self.dist_token.shape\", dist_token.shape)\n",
    "            x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "\n",
    "        if first_RUN: print(\" final sequence x\", x.shape)\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks(x)\n",
    "        if first_RUN: print(f\" after {len(self.blocks)} atten blocks x\", x.shape)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        global first_RUN\n",
    "        if first_RUN: print(\"x\", x.size())\n",
    "\n",
    "        x = self.forward_features(x)\n",
    "\n",
    "        if self.head_dist is not None:\n",
    "            features = (x[0] + x[1]) / 2\n",
    "            if first_RUN: print(\"forward_features\", features.size())\n",
    "            x = self.head(features)\n",
    "            if first_RUN: print(\"head\", x.size())\n",
    "            first_RUN = False\n",
    "            return x, features\n",
    "        else:\n",
    "            features = x\n",
    "            if first_RUN: print(\"forward_features\", features.size())\n",
    "            x = self.head(x)\n",
    "        if first_RUN: print(\"head\", x.size())\n",
    "        first_RUN = False\n",
    "        return x, features\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=(), mode='bicubic'):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s with %s cls/dis tokens', posemb.shape, posemb_new.shape,\n",
    "                 num_tokens)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode=mode, align_corners=False)\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def adapt_image_pos_embed_to_passt(posemb, num_tokens=1, gs_new=(), mode='bicubic'):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s with %s cls/dis tokens', posemb.shape, gs_new,\n",
    "                 num_tokens)\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode=mode, align_corners=False)\n",
    "    freq_new_pos_embed = posemb_grid.mean(dim=3, keepdim=True)\n",
    "    time_new_pos_embed = posemb_grid.mean(dim=2, keepdim=True)\n",
    "    _logger.info('New Position cls/dstl embedding %s', posemb_tok.shape)\n",
    "    _logger.info('New FREQ Position embedding %s', freq_new_pos_embed.shape)\n",
    "    _logger.info('New TIME Position embedding %s', time_new_pos_embed.shape)\n",
    "    return posemb_tok, freq_new_pos_embed, time_new_pos_embed\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    state_dict = {k: v for k, v in state_dict.items()}\n",
    "    if \"time_new_pos_embed\" not in state_dict:\n",
    "        # we are working with ImageNet model\n",
    "        _logger.info(\"Adapting pos embedding from ImageNet pretrained model to PaSST.\")\n",
    "        v = state_dict.pop(\"pos_embed\")\n",
    "        new_pos_embed, freq_new_pos_embed, time_new_pos_embed = adapt_image_pos_embed_to_passt(\n",
    "            v, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        state_dict[\"new_pos_embed\"] = new_pos_embed\n",
    "        state_dict[\"freq_new_pos_embed\"] = freq_new_pos_embed\n",
    "        state_dict[\"time_new_pos_embed\"] = time_new_pos_embed\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # this should never occur\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8442d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from timm.models.helpers import load_pretrained\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def overlay_external_default_cfg(default_cfg, kwargs):\n",
    "    \"\"\" Overlay 'external_default_cfg' in kwargs on top of default_cfg arg.\n",
    "    \"\"\"\n",
    "    external_default_cfg = kwargs.pop('external_default_cfg', None)\n",
    "    if external_default_cfg:\n",
    "        default_cfg.pop('url', None)  # url should come from external cfg\n",
    "        default_cfg.pop('hf_hub', None)  # hf hub id should come from external cfg\n",
    "        default_cfg.update(external_default_cfg)\n",
    "\n",
    "\n",
    "def filter_kwargs(kwargs, names):\n",
    "    if not kwargs or not names:\n",
    "        return\n",
    "    for n in names:\n",
    "        kwargs.pop(n, None)\n",
    "\n",
    "\n",
    "def set_default_kwargs(kwargs, names, default_cfg):\n",
    "    for n in names:\n",
    "        # for legacy reasons, model __init__args uses img_size + in_chans as separate args while\n",
    "        # default_cfg has one input_size=(C, H ,W) entry\n",
    "        if n == 'img_size':\n",
    "            input_size = default_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[-2:])\n",
    "        elif n == 'in_chans':\n",
    "            input_size = default_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[0])\n",
    "        else:\n",
    "            default_val = default_cfg.get(n, None)\n",
    "            if default_val is not None:\n",
    "                kwargs.setdefault(n, default_cfg[n])\n",
    "\n",
    "\n",
    "def update_default_cfg_and_kwargs(default_cfg, kwargs, kwargs_filter):\n",
    "    \"\"\" Update the default_cfg and kwargs before passing to model\n",
    "    FIXME this sequence of overlay default_cfg, set default kwargs, filter kwargs\n",
    "    could/should be replaced by an improved configuration mechanism\n",
    "    Args:\n",
    "        default_cfg: input default_cfg (updated in-place)\n",
    "        kwargs: keyword args passed to model build fn (updated in-place)\n",
    "        kwargs_filter: keyword arg keys that must be removed before model __init__\n",
    "    \"\"\"\n",
    "    # Overlay default cfg values from `external_default_cfg` if it exists in kwargs\n",
    "    overlay_external_default_cfg(default_cfg, kwargs)\n",
    "    # Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)\n",
    "    default_kwarg_names = ('num_classes', 'global_pool', 'in_chans')\n",
    "    if default_cfg.get('fixed_input_size', False):\n",
    "        # if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size\n",
    "        default_kwarg_names += ('img_size',)\n",
    "    set_default_kwargs(kwargs, names=default_kwarg_names, default_cfg=default_cfg)\n",
    "    # Filter keyword args for task specific model variants (some 'features only' models, etc.)\n",
    "    filter_kwargs(kwargs, names=kwargs_filter)\n",
    "\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    if mode == 'fan_in':\n",
    "        denom = fan_in\n",
    "    elif mode == 'fan_out':\n",
    "        denom = fan_out\n",
    "    elif mode == 'fan_avg':\n",
    "        denom = (fan_in + fan_out) / 2\n",
    "\n",
    "    variance = scale / denom\n",
    "\n",
    "    if distribution == \"truncated_normal\":\n",
    "        # constant is stddev of standard normal truncated to (-2, 2)\n",
    "        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)\n",
    "    elif distribution == \"normal\":\n",
    "        tensor.normal_(std=math.sqrt(variance))\n",
    "    elif distribution == \"uniform\":\n",
    "        bound = math.sqrt(3 * variance)\n",
    "        tensor.uniform_(-bound, bound)\n",
    "    else:\n",
    "        raise ValueError(f\"invalid distribution {distribution}\")\n",
    "\n",
    "\n",
    "def lecun_normal_(tensor):\n",
    "    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')\n",
    "\n",
    "\n",
    "\n",
    "def build_model_with_cfg(\n",
    "        model_cls,\n",
    "        variant: str,\n",
    "        pretrained: bool,\n",
    "        default_cfg: dict,\n",
    "        model_cfg= None,\n",
    "        feature_cfg= None,\n",
    "        pretrained_strict: bool = True,\n",
    "        pretrained_filter_fn = None,\n",
    "        pretrained_custom_load = False,\n",
    "        kwargs_filter = None,\n",
    "        **kwargs):\n",
    "    \"\"\" Build model with specified default_cfg and optional model_cfg\n",
    "    This helper fn aids in the construction of a model including:\n",
    "      * handling default_cfg and associated pretained weight loading\n",
    "      * passing through optional model_cfg for models with config based arch spec\n",
    "      * features_only model adaptation\n",
    "      * pruning config / model adaptation\n",
    "    Args:\n",
    "        model_cls (nn.Module): model class\n",
    "        variant (str): model variant name\n",
    "        pretrained (bool): load pretrained weights\n",
    "        default_cfg (dict): model's default pretrained/task config\n",
    "        model_cfg (Optional[Dict]): model's architecture config\n",
    "        feature_cfg (Optional[Dict]: feature extraction adapter config\n",
    "        pretrained_strict (bool): load pretrained weights strictly\n",
    "        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n",
    "        pretrained_custom_load (bool): use custom load fn, to load numpy or other non PyTorch weights\n",
    "        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n",
    "        **kwargs: model args passed through to model __init__\n",
    "    \"\"\"\n",
    "    pruned = kwargs.pop('pruned', False)\n",
    "    features = False\n",
    "    feature_cfg = feature_cfg or {}\n",
    "    default_cfg = deepcopy(default_cfg) if default_cfg else {}\n",
    "    update_default_cfg_and_kwargs(default_cfg, kwargs, kwargs_filter)\n",
    "    default_cfg.setdefault('architecture', variant)\n",
    "\n",
    "    # Setup for feature extraction wrapper done at end of this fn\n",
    "    if kwargs.pop('features_only', False):\n",
    "        features = True\n",
    "        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n",
    "        if 'out_indices' in kwargs:\n",
    "            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n",
    "\n",
    "    # Build the model\n",
    "    model = model_cls(**kwargs) if model_cfg is None else model_cls(cfg=model_cfg, **kwargs)\n",
    "    model.default_cfg = default_cfg\n",
    "\n",
    "\n",
    "    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "    if pretrained:\n",
    "        if pretrained_custom_load:\n",
    "            load_custom_pretrained(model)\n",
    "        else:\n",
    "            load_pretrained(\n",
    "                model,\n",
    "                num_classes=num_classes_pretrained,\n",
    "                in_chans=kwargs.get('in_chans', 3),\n",
    "                filter_fn=pretrained_filter_fn,\n",
    "                strict=pretrained_strict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d05c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PaSST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9d6a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaSST(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(64, 768, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PaSST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90ed0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae0b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "# import cv2\n",
    "import os\n",
    "user_path = os.path.expanduser('~')\n",
    "EPS = 1e-16\n",
    "\n",
    "\n",
    "class VideoFrontend(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoFrontend, self).__init__()\n",
    "        self.video_frontend = nn.Sequential(\n",
    "            nn.Conv3d(1, 3, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False),\n",
    "            nn.BatchNorm3d(3), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)))\n",
    "        #self.resnet = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=output_dim)\n",
    "        self.passt = PaSST()\n",
    "    def forward(self, x):\n",
    "        B, T, _, _ = x.size()\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.video_frontend(x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = x.view(-1, 3, x.size(3), x.size(4))\n",
    "        x = self.passt(x)\n",
    "        x = x[0]\n",
    "        x = x.view(B, T, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.bnfc = nn.BatchNorm1d(num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        print(x.shape)\n",
    "        x = self.bnfc(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de68348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 96, 96)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f41ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =dataset_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, _, _ = x.size()\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.video_frontend(x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = x.view(-1, 64, x.size(3), x.size(4))\n",
    "        x = self.passt(x)\n",
    "        x = x.view(B, T, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa2930ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5137c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8849c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 512])\n",
      "torch.Size([34, 256])\n",
      "torch.Size([34, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6305e-01,  6.5810e-01,  2.3121e+00,  ...,  4.9449e-02,\n",
       "          1.2323e+00,  1.3723e+00],\n",
       "        [ 1.6004e+00,  1.2604e-02,  3.8126e-01,  ..., -1.4543e-01,\n",
       "         -8.7489e-01, -8.2432e-02],\n",
       "        [-2.9362e-01,  1.1488e+00,  1.0416e+00,  ..., -8.6923e-01,\n",
       "          1.2141e+00, -8.8083e-01],\n",
       "        ...,\n",
       "        [-5.6258e-01, -1.1120e+00,  2.9112e-03,  ...,  1.1078e+00,\n",
       "          2.2083e+00, -2.9774e-01],\n",
       "        [ 3.4067e+00,  3.2376e+00, -1.1747e+00,  ..., -1.4241e-01,\n",
       "          8.0649e-01,  1.2649e+00],\n",
       "        [ 1.0837e+00,  7.9165e-02,  1.2600e+00,  ...,  1.8034e+00,\n",
       "         -1.4604e+00, -2.6419e-02]], device='cuda:0',\n",
       "       grad_fn=<CudnnBatchNormBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ea99c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 64, 24, 24])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c3043f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 512])\n",
      "torch.Size([22, 1])\n",
      "torch.Size([22, 1])\n"
     ]
    }
   ],
   "source": [
    "ouput= resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d952412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a960b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Passt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-837128abbd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPasst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Passt' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8545327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoFrontend(\n",
       "  (video_frontend): Sequential(\n",
       "    (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (resnet): ResNet(\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (bnfc): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VideoFrontend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a19e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cbbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd78e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3b971b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-78a5c4213f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model parameter 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'name:{name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'param.shape:{param.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nnet' is not defined"
     ]
    }
   ],
   "source": [
    "#model parameter 확인\n",
    "for name, param in nnet.named_parameters(): \n",
    "    print(f'name:{name}') \n",
    "    print(type(param)) \n",
    "    print(f'param.shape:{param.shape}') \n",
    "    print(f'param.requries_grad:{param.requires_grad}') \n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b41219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da148d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47422e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9f2b259887ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff38ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(-1, 64, x.size(3), x.size(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "048d7833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 64, 24, 24])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b27abc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4430b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "743b4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd62d917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 256])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b271a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(B, T, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7bf6d7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 9216])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "773f9649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,t,96*96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cf058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf70308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf580f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf6e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9f7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd5dc552",
   "metadata": {},
   "outputs": [],
   "source": [
    " x = x.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4058084a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 64, 96, 96]' is invalid for input of size 313344",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c8b107037af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 64, 96, 96]' is invalid for input of size 313344"
     ]
    }
   ],
   "source": [
    "x = x.view(-1, 64, x.size(3), x.size(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92917b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c3f55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "#from extract_lip_embedding_resnet import VideoFrontend\n",
    "\n",
    "class LSTM_Encoder(nn.Module):\n",
    "    def __init__(self,feature_dim,hidden_size,num_layers):\n",
    "        super(LSTM_Encoder, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.stack_rnn = nn.LSTM(input_size=self.feature_dim, hidden_size=self.hidden_size, batch_first=False, bidirectional=False, num_layers=1)\n",
    "\n",
    "    def forward(self, cur_inputs, current_frame):\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(cur_inputs, current_frame)\n",
    "        rnn_out, _ = self.stack_rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out) \n",
    "               \n",
    "        return rnn_out\n",
    "\n",
    "\n",
    "class KWS_Net(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_sizes,lstm_num_layers):\n",
    "        super(KWS_Net,self).__init__()\n",
    "        self.lip_encoder = VideoFrontend()\n",
    "        # self.conv_av1 = nn.Conv2d(256,256, kernel_size=(1, 5), stride=(1,2), padding=(0,2))\n",
    "        # self.conv_av2 = nn.Conv2d(256,512, kernel_size=(1, 5), stride=(1,2), padding=(0,2))\n",
    "        self.feature_dim = input_dim\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.num_layers = lstm_num_layers\n",
    "       \n",
    "        self.encoder = LSTM_Encoder(256, self.hidden_size, self.num_layers)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,32, kernel_size=(5,5), stride=(1,1), padding=(2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=(5,5), stride=(2,2), padding=(2,2))\n",
    "        self.max_pool1 = nn.MaxPool2d(2, stride=(1,2))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(5,5), stride=(1,2), padding=(2,2))\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "       \n",
    "\n",
    "    def forward(self, video_inputs, current_frame):\n",
    "        video_inputs = video_inputs.permute(1, 0, 2, 3)\n",
    "        lip_inputs = self.lip_encoder(video_inputs) #(B, T, 256)\n",
    "        lip_inputs = lip_inputs.permute(1, 0, 2) # (T, B, 256)\n",
    "        \n",
    "        # #lstm layer\n",
    "        encoder_output = self.encoder(lip_inputs, current_frame)\n",
    "        encoder_output = encoder_output.permute(1,2,0) \n",
    "        # print(encoder_output.shape) # (B,64,T)\n",
    "        \n",
    "        # #CNN detector and classifier \n",
    "        cnn_input = encoder_output.unsqueeze(1) \n",
    "        cnn_out = self.conv1(cnn_input)\n",
    "        cnn_out = self.conv2(cnn_out)\n",
    "        cnn_out = self.max_pool1(cnn_out)\n",
    "        cnn_out = ((self.conv3(cnn_out)).mean(-2)).permute(0,2,1) \n",
    "        cnn_out = self.fc1(cnn_out)\n",
    "        cnn_out = self.dropout(cnn_out)\n",
    "        cnn_out = self.fc2(cnn_out)\n",
    "        cnn_out = self.fc3(cnn_out)\n",
    "\n",
    "        max_pool2 = nn.MaxPool2d((cnn_out.shape[1],1))\n",
    "        cnn_out = max_pool2(cnn_out) \n",
    "        cnn_out = cnn_out.squeeze(-1)\n",
    " \n",
    "        return cnn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7603c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = KWS_Net(input_dim=256, hidden_sizes=256,lstm_num_layers=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b626f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bd5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#swa\n",
    "from torchcontrib.optim import SWA\n",
    "import torchcontrib\n",
    "\n",
    "base_opt = torch.optim.Adam(filter(lambda p: p.requires_grad, nnet.parameters()), lr=5e-5, weight_decay=1e-6)\n",
    "optimizer = torchcontrib.optim.SWA(base_opt, swa_start=10, swa_freq=5, swa_lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea824a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KWS_Net(\n",
       "  (lip_encoder): VideoFrontend(\n",
       "    (video_frontend): Sequential(\n",
       "      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (passt): PaSST(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(64, 768, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=768, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): LSTM_Encoder(\n",
       "    (stack_rnn): LSTM(256, 256)\n",
       "  )\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 2), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a039aadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaSST(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(64, 768, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PaSST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(nnet.parameters(), lr=5e-5)\n",
    "BCE_loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7445d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWS_Net(\n",
      "  (lip_encoder): VideoFrontend(\n",
      "    (video_frontend): Sequential(\n",
      "      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (passt): PaSST(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(64, 768, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): Sequential(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pre_logits): Identity()\n",
      "      (head): Sequential(\n",
      "        (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): LSTM_Encoder(\n",
      "    (stack_rnn): LSTM(256, 256)\n",
      "  )\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 2), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf2afa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "    \n",
    "nnet = KWS_Net(input_dim=256, hidden_sizes=256,lstm_num_layers=1).cuda()\n",
    "    #nnet = nnet.cuda()\n",
    "\n",
    "#pretrained_dict =torch.load('/home/hj20/misp2021_baseline/task1_wws/lipreading_LRW.pt', map_location=torch.device(\"cuda\"))\n",
    "#model_dict = nnet.lip_encoder.state_dict()\n",
    "#new_model_dict = dict()\n",
    "#for k, v in model_dict.items():\n",
    "#    if ('video_frontend.' + k) in pretrained_dict.keys() and v.size() == pretrained_dict['video_frontend.' + k].size():\n",
    "#        new_model_dict[k] = pretrained_dict['video_frontend.' + k]\n",
    "#new_model_dict = {k: v for k, v in new_model_dict.items()\n",
    "#                if k in model_dict.keys()\n",
    "#                and v.size() == model_dict[k].size()}\n",
    "#nnet.lip_encoder.load_state_dict(new_model_dict)\n",
    "\n",
    "    # training setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737099ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KWS_Net(\n",
       "  (lip_encoder): VideoFrontend(\n",
       "    (video_frontend): Sequential(\n",
       "      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (passt): PaSST(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(64, 768, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=768, out_features=527, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): LSTM_Encoder(\n",
       "    (stack_rnn): LSTM(256, 256)\n",
       "  )\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 2), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f4714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edbb6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([300, 3, 24, 24])\n",
      "self.norm(x) torch.Size([300, 768, 6, 6])\n",
      " patch_embed :  torch.Size([300, 768, 6, 6])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 6])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 6, 1])\n",
      "X flattened torch.Size([300, 36, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 1, 768])\n",
      " self.cls_tokens.shape torch.Size([300, 1, 768])\n",
      " final sequence x torch.Size([300, 37, 768])\n",
      " after 12 atten blocks x torch.Size([300, 37, 768])\n",
      "forward_features torch.Size([300, 768])\n",
      "head torch.Size([300, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 77.69 MiB free; 8.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-aafdab9fef4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 77.69 MiB free; 8.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for iter_ in range(20):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    nnet.train()\n",
    "    for video_feature, data_label_torch, current_frame in dataloader_train:\n",
    "        video_feature = video_feature.cuda()\n",
    "    \n",
    "        data_label_torch = data_label_torch.cuda()\n",
    "        outputs = nnet(video_feature, current_frame)\n",
    "        loss = BCE_loss(outputs, data_label_torch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    logger.info(\"Iteration:{0}, loss = {1:.6f} \".format(iter_, running_loss / all_file))\n",
    "\n",
    "        # eval\n",
    "    nnet.eval()\n",
    "    pre_list, pre_list_d, label_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        running_loss_dev = 0.0\n",
    "        pre_sum = 0.0\n",
    "        for video_feature_dev, data_label_torch_dev, current_frame_dev in dataloader_dev:\n",
    "            video_feature_dev = video_feature_dev.cuda()\n",
    "            data_label_torch_dev = data_label_torch_dev.cuda()\n",
    "            outputs_dev = nnet(video_feature_dev, current_frame_dev)\n",
    "            loss_dev = BCE_loss(outputs_dev, data_label_torch_dev)\n",
    "                \n",
    "            running_loss_dev += loss_dev.item()\n",
    "            outputs_dev_np = (torch.ceil(torch.sigmoid(outputs_dev)-0.5)).data.cpu().numpy()\n",
    "            pre_list.extend(outputs_dev_np[:,0])\n",
    "            label_list.extend(list(data_label_torch_dev.data.cpu().numpy()))\n",
    "                \n",
    "        TP, FP, TN, FN = utils.cal_indicator(pre_list, label_list)\n",
    "\n",
    "        FAR = FP / (FP + TN)\n",
    "        FRR = 1 - TP / (TP + FN)\n",
    "\n",
    "        logger.info(\"Middle video Epoch:%d, Train loss=%.4f, Valid loss=%.4f, FAR=%.4f, FRR:%.4f\" % (iter_,\n",
    "                    running_loss / all_file,running_loss_dev /all_file_dev, FAR,FRR))\n",
    "\n",
    "    torch.save(nnet.state_dict(), os.path.join(model_path, \"{}_{}.model\".format('kws_video', iter_)))\n",
    "    nnet.train()\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(\"Time used for each epoch training: {} seconds.\".format(end_time - start_time))\n",
    "    logger.info(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27dc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8270eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 16, 96, 96])torch.Size([75, 16, 96, 96])\n",
      "\n",
      "torch.Size([75, 16, 96, 96])torch.Size([75, 16, 96, 96])torch.Size([75, 16, 96, 96])\n",
      "\n",
      "\n",
      "torch.Size([75, 16, 96, 96])\n",
      "torch.Size([75, 16, 96, 96])\n",
      "torch.Size([75, 16, 96, 96])\n",
      "torch.Size([75, 16, 96, 96])\n",
      "torch.Size([75, 16, 96, 96])torch.Size([75, 16, 96, 96])\n",
      "\n",
      "torch.Size([75, 16, 96, 96])\n",
      "torch.Size([75, 16, 96, 96]) torch.Size([16, 1]) 16\n",
      "[75, 75, 75, 75, 75, 71, 66, 50, 49, 44, 36, 34, 32, 30, 26, 25]\n"
     ]
    }
   ],
   "source": [
    "video_feature, data_label_torch, current_frame = next(iter(dataloader_train))\n",
    "print(video_feature.shape, data_label_torch.shape, len(current_frame))\n",
    "print(current_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\", default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--minibatchsize_train\", default=16, type=int)\n",
    "parser.add_argument(\"--minibatchsize_dev\", default=1, type=int)\n",
    "parser.add_argument(\"--input_dim\", default=256, type=int)\n",
    "parser.add_argument(\"--hidden_sizes\", default=256, type=int)\n",
    "parser.add_argument(\"--output_dim\", default=1, type=int)\n",
    "parser.add_argument(\"--lstm_num_layers\", default=1, type=int)\n",
    "parser.add_argument(\"--seed\", default=617, type=int)\n",
    "parser.add_argument(\"--project\", default='video_model', type=str)\n",
    "parser.add_argument(\"--logdir\", default='./log/', type=str)\n",
    "parser.add_argument(\"--model_name\", default='kws_video', type=str)\n",
    "parser.add_argument(\"--start_iter\", default=0, type=int)\n",
    "parser.add_argument(\"--end_iter\", default=20, type=int)\n",
    "parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
    "parser.add_argument(\"--train_num_workers\", default=6, type=int, help=\"number of training workers\")\n",
    "parser.add_argument(\"--dev_num_workers\", default=1, type=int, help=\"number of validation workers\")\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "101d5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit-pytorch\n",
      "  Downloading vit_pytorch-0.26.3-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 819 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.3 in /home/hj20/anaconda3/lib/python3.7/site-packages (from vit-pytorch) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.6 in /home/hj20/anaconda3/lib/python3.7/site-packages (from vit-pytorch) (1.8.1)\n",
      "Requirement already satisfied: torchvision in /home/hj20/anaconda3/lib/python3.7/site-packages (from vit-pytorch) (0.9.1)\n",
      "Requirement already satisfied: numpy in /home/hj20/anaconda3/lib/python3.7/site-packages (from torch>=1.6->vit-pytorch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /home/hj20/anaconda3/lib/python3.7/site-packages (from torch>=1.6->vit-pytorch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/hj20/anaconda3/lib/python3.7/site-packages (from torchvision->vit-pytorch) (8.2.0)\n",
      "Installing collected packages: vit-pytorch\n",
      "Successfully installed vit-pytorch-0.26.3\n"
     ]
    }
   ],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74f0aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (36x1024 and 48x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dc8164e7794b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (1, 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/vit_pytorch/vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_patch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (36x1024 and 48x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 24,\n",
    "    patch_size = 4,\n",
    "    num_classes = 256,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 64, 24, 24)\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92c122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
